extern strlen = (*i8) => i64;
extern memcpy = (*void, *void, i64) => void;
extern sprintf = (*i8, *i8, varargs) => void;
extern atoi = (*i8) => i64;
extern fopen = (*i8, *i8) => *i8;
extern fgets = (*i8, i64, *i8) => void;
extern feof = (*i8) => bool;
extern fseek = (*i8, i64, i64) => i64;
extern ftell = (*i8) => i64;
extern fread = (*i8, i64, i64, *i8) => i64;
extern fclose = (*i8) => *i8;

import "!stdlib.src";
import "!mem.src";

let tokenizer = struct {
	tokens: *i8,
	tokens_len: i64,
	buf: *i8,
	file_size: i64,
	offset: i64,

	arena: *arena,
};

let read_file = (t: *tokenizer, filename: *i8) => void {
	let file = fopen(filename, "r");

	fseek(file, 0, 2);
	(*t).file_size = ftell(file);
	fseek(file, 0, 0);

	let buf = cast(*i8, arena_alloc((*t).arena, (*t).file_size + 1));

	let bytes_read = fread(buf, 1, (*t).file_size, file);
	(*(buf + cast(*i8, bytes_read))) = '\0';

	fclose(file);

	(*t).buf = buf;

	return;
};

let add_token = (t: *tokenizer, token: *i8) => i64 {
	println("Add token: %s", token);
	let i = 0;
	while true {
		let c = (*(token + cast(*i8, i)));

		(*((*t).tokens + cast(*i8, (*t).tokens_len))) = c;

		(*t).tokens_len = (*t).tokens_len + 1;
		i = i + 1;

		if c == '\0' {
			return 0;
		};
	};

	return 0;
};

let print_tokens = (t: *tokenizer) => i64 {
	let i = 0;
	while i < (*t).tokens_len {
		let c = (*((*t).tokens + cast(*i8, i)));
		if c == '\0' {
			c = '\n';
		};

		printf("%c", c);

		i = i + 1;
	};

	return 0;
};

let tokenizer_skip_whitespace = (t: *tokenizer) => void {
	while true {
		if (*t).offset >= (*t).file_size { return; };
		let c = (*((*t).buf + cast(*i8, (*t).offset)));
		if !iswhitespace(c) {
			return;
		};
		(*t).offset = (*t).offset + 1;
	};

	return;
};

let tokenizer_accept_string = (t: *tokenizer, str: *i8) => bool {
	let str_len = strlen(str);
	if (*t).offset + str_len > (*t).file_size { return false; };

	let s = cast(*i8, arena_alloc((*t).arena, 1000));
	memcpy(cast(*void, s), cast(*void, (*t).buf + cast(*i8, (*t).offset)), str_len);

	if strcmp(s, str) {
		(*t).offset = (*t).offset + str_len;
		return true;
	};

	return false;
};

let tokenizer_consume_until_condition = (t: *tokenizer, condition: (i8) => bool) => *i8 {
	let start = (*t).offset;
	let res = cast(*i8, arena_alloc((*t).arena, 1000));

	while true {
		if (*t).offset >= (*t).file_size {
			return res;
		};

		let c = (*((*t).buf + cast(*i8, (*t).offset)));

		let offset = (*t).offset;
		if c == '\\' {
			let next_c = (*((*t).buf + cast(*i8, offset + 1)));
		
			let any = false;
			if next_c == 'n' {
				(*(res + cast(*i8, offset - start))) = '\n';
				any = true;
			};
			if next_c == 't' {
				(*(res + cast(*i8, offset - start))) = '\t';
				any = true;
			};
			if next_c == 'r' {
				(*(res + cast(*i8, offset - start))) = '\r';
				any = true;
			};
			if next_c == '0' {
				(*(res + cast(*i8, offset - start))) = '\0';
				any = true;
			};
			if next_c == '\\' {
				(*(res + cast(*i8, offset - start))) = '\\';
				any = true;
			};
			if !any {
				(*(res + cast(*i8, offset - start))) = next_c;
			};
			
			offset = offset + 1;
			offset = offset + 1;
			(*t).offset = offset;

			continue;
		};

		if condition(c) {
			return res;
		};
		
		(*(res + cast(*i8, offset - start))) = c;
		(*(res + cast(*i8, offset - start + 1))) = '\0';

		offset = offset + 1;
		(*t).offset = offset;
	};

	return cast(*i8, null);
};

let tokenizer_accept_int_type = (t: *tokenizer) => *i64 {
	let string = tokenizer_consume_until_condition(t, (c: i8) => bool {
		return !isdigit(c);
	});
	if string == cast(*i8, null) {
		return cast(*i64, null);
	};
	if strlen(string) == 0 {
		return cast(*i64, null);
	};
	let x = cast(*i64, arena_alloc((*t).arena, 8));
	*x = atoi(string);
	return x;
};

let tokenizer_accept_char_type = (t: *tokenizer) => *i8 {
	let prev_offset = (*t).offset;
	if !tokenizer_accept_string(t, "'") {
		(*t).offset = prev_offset;
		return cast(*i8, null);
	};

	let string = tokenizer_consume_until_condition(t, (c: i8) => bool {
		return c == '\'';
	});

	if !tokenizer_accept_string(t, "'") {
		(*t).offset = prev_offset;
		return cast(*i8, null);
	};

	return string;
};

let tokenizer_accept_string_type = (t: *tokenizer) => *i8 {
	let prev_offset = (*t).offset;
	if !tokenizer_accept_string(t, "\"") {
		(*t).offset = prev_offset;
		return cast(*i8, null);
	};

	let string = tokenizer_consume_until_condition(t, (c: i8) => bool {
		return c == '"';
	});

	if !tokenizer_accept_string(t, "\"") {
		(*t).offset = prev_offset;
		return cast(*i8, null);
	};

	return string;
};

let tokenizer_skip_comments = (t: *tokenizer) => void {
	if !tokenizer_accept_string(t, "/*") { return; };

	while !tokenizer_accept_string(t, "*/") {
		(*t).offset = (*t).offset + 1;
	};

	return;
};

let tokenizer_next = (t: *tokenizer) => *i8 {
	tokenizer_skip_whitespace(t);
	tokenizer_skip_comments(t);
	tokenizer_skip_whitespace(t);

	if (*t).offset >= (*t).file_size {
		return "EOF";
	};

	if tokenizer_accept_string(t, "import") {
	    return "import";
	};
	if tokenizer_accept_string(t, "let") {
	    return "let";
	};
	if tokenizer_accept_string(t, "extern") {
	    return "extern";
	};
	if tokenizer_accept_string(t, "if") {
	    return "if";
	};
	if tokenizer_accept_string(t, "while") {
	    return "while";
	};
	if tokenizer_accept_string(t, "return") {
	    return "return";
	};
	if tokenizer_accept_string(t, "break") {
	    return "break";
	};
	if tokenizer_accept_string(t, "true") {
	    return "bool:true";
	};
	if tokenizer_accept_string(t, "false") {
	    return "bool:false";
	};

	if tokenizer_accept_string(t, "=>") {
	    return "=>";
	};
	if tokenizer_accept_string(t, ";") {
	    return ";";
	};
	if tokenizer_accept_string(t, ",") {
	    return ",";
	};
	if tokenizer_accept_string(t, ":") {
	    return ":";
	};
	if tokenizer_accept_string(t, "(") {
	    return "(";
	};
	if tokenizer_accept_string(t, ")") {
	    return ")";
	};
	if tokenizer_accept_string(t, "{") {
	    return "{";
	};
	if tokenizer_accept_string(t, "}") {
	    return "}";
	};
	if tokenizer_accept_string(t, "=") {
	    return "=";
	};
	if tokenizer_accept_string(t, "+") {
	    return "+";
	};
	if tokenizer_accept_string(t, "-") {
	    return "-";
	};
	if tokenizer_accept_string(t, "*") {
	    return "*";
	};
	if tokenizer_accept_string(t, "/") {
	    return "/";
	};
	if tokenizer_accept_string(t, "%") {
	    return "%";
	};
	if tokenizer_accept_string(t, "!") {
	    return "!";
	};
	if tokenizer_accept_string(t, "<") {
	    return "<";
	};
	if tokenizer_accept_string(t, ">") {
	    return ">";
	};
	if tokenizer_accept_string(t, ".") {
	    return ".";
	};
	
	let maybe_int = tokenizer_accept_int_type(t);
	if maybe_int != cast(*i64, null) {
		let to = cast(*i8, arena_alloc((*t).arena, 1000));
		sprintf(to, "int:%d", *maybe_int);

		return to;
	};

	let maybe_char = tokenizer_accept_char_type(t);
	if maybe_char != cast(*i8, null) {
		let to = cast(*i8, arena_alloc((*t).arena, 1000));
		sprintf(to, "char:%d", *maybe_char);

		return to;
	};

	let maybe_string = tokenizer_accept_string_type(t);
	if maybe_string != cast(*i8, null) {
		let to = cast(*i8, arena_alloc((*t).arena, 1000));
		sprintf(to, "string:%s", maybe_string);

		return to;
	};

	let string = tokenizer_consume_until_condition(t, (c: i8) => bool {
		if isalphanum(c) {
			return false;
		};
		if c == '_' {
			return false;
		};
		return true;
	});
	if strlen(string) == 0 {
		return cast(*i8, null);
	};

	let to = cast(*i8, arena_alloc((*t).arena, 100));
	sprintf(to, "identifier:%s", string);
	
	return to;
};

let tokenizer_init = (alloc: *arena, filename: *i8) => i64 {
	let t = cast(*tokenizer, arena_alloc(alloc, sizeof(tokenizer)));
	(*t).arena = alloc;
	(*t).tokens = cast(*i8, arena_alloc((*t).arena, 100000));

	read_file(t, filename);

	println("File size: %d", (*t).file_size);

	println("%s", (*t).buf);


	while true {
		let tk = tokenizer_next(t);
		if tk == cast(*i8, null) {
			println("NULL TOKEN!");
			return 1;
		};
		if strcmp(tk, "EOF") {
			break;
		};
		add_token(t, tk);
	};

	println("PRINT TOKENS");

	print_tokens(t);

	return 0;
};

let tokenizer_deinit = () => i64 {
	return 0;
};
